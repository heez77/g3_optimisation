{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnjzW0-TPMvF"
   },
   "source": [
    "# __Méthode de gradient stochastique pour l'apprentissage d'un réseau de neurones sur un problème jouet de classification .__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ie13pHt3gyh8"
   },
   "source": [
    "<h1><a id='toc'></a>Sommaire</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ul>\n",
    "        <li><a href=\"#0\">0. Données du problème d'optimisation </a></li>\n",
    "        <li><a href=\"#I\">1. Implémentation de la méthode du gradient stochastique à taux d'apprentissage constant (rappel) </a></li>\n",
    "        <li><a href=\"#II\">2. Réduction du bruit par décroissance du taux d'apprentissage</a></li> \n",
    "        <li><a href=\"#III\">3. Méthodes des mini-lots (mini-batch method)</a></li> \n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from toynn_2023 import *\n",
    "# charge aussi les bibliothèques suivantes :\n",
    "#    import numpy as np\n",
    "#    from numpy import random as nprd\n",
    "#    from matplotlib import pyplot as plt\n",
    "#    from matplotlib import cm as cm\n",
    "#    from copy import deepcopy as dcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id='0'></a>\n",
    "0.  Données du problème d'optimisation </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#toc\">top</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#0\">0</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#I\">1.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#II\">2.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#III\">3.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#bot\">bot.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Woacru-MVEgA"
   },
   "source": [
    "### Sélection d'un problème ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = ToyPb(name = \"sin\", bounds = (-1,1))\n",
    "pb.show_border()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un ensemble de données taguées à l'aide du problème précédent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = 1000\n",
    "data = nD_data(n = ndata, pb = pb)\n",
    "\n",
    "data.show_class()\n",
    "pb.show_border('k--')\n",
    "plt.legend(loc=1, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_pb_ et _data_ étant donnés, on souhaite fabriquer une fonction $h$ parmi un ensemble de fonctions de la forme $h(\\cdot;A)$ où le paramètre $A$ évolue librement dans $\\mathbb{R}^N$ qui minimise\n",
    "$$\n",
    "F(A):=\\dfrac1{n_d}\\sum_{i=0}^{n_d-1}\\ell(h(x^i;A)y^i)=\\,\\dfrac1{n_d}\\sum_{i=0}^{n_d-1}f_i(A).\n",
    "$$\n",
    "où $n_d=$*data.n*, $x^i=$*data.X*[i], $y^i=$*data.Y*[i] et $\\ell=$*pb.loss*. \n",
    "\n",
    "Plus bas on fixe un type de réseau de neurones dont les paramètres $A$ sont les poids associés aux arrêtes et aux noeuds.  Ces réseaux de neurones ont deux noeud d'entrées auxquels on assignera les valeurs $x^i_0,x^i_1$ et un noeud en sortie qui produira la valeur $h((x^i_0,x^i_1;A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un type de réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le type de réseau de neurones est caractérisé par le nombre de couches et le nombre de noeuds dans chaque couche ainsi que par une fonction d'activation $\\chi$.<br>\n",
    "Ici on choisit 5 couches avec respectivement 2, 4, 6, 4 et 1 noeuds et on prend  $\\chi(t)=tanh(t)$.\n",
    "\n",
    "Le paramètre _grid=(-1,1,41)_ sert pour les représentation graphiques de la sortie produite par un réseau de neurones pour les entrées $(x_i,y_j)$ parcourant les noeuds de la grille régulière obtenue par discrétisation du carré $[-1,1]\\times[-1,1]$ avec le pas $h=1/20$.\n",
    "$$\n",
    "x_i=-1+ih,\\qquad y_j=-1+jh\\qquad\\text{ avec }0\\le i,j\\le 40.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CardNodes = (2, 5, 5, 5, 1)\n",
    "nn = ToyNN(card = CardNodes, chi=\"tanh\", grid=(-1,1,41))\n",
    "\n",
    "# Exemple de coefficients et représentation\n",
    "A= nn.create_rand()\n",
    "nn.show(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GafO0zXoJ6Cx"
   },
   "source": [
    "<h2><a id='I'></a>\n",
    "1.  Implémentation de la méthode du gradient stochastique à taux d'apprentissage constant </h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#toc\">top</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#0\">0</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#I\">1.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#II\">2.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#III\">3.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#bot\">bot.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un pas de la méthode de gradient stochastique consiste à choisir aléatoirement (et uniformément) $i_k\\in\\{0,\\dots,n_d-1\\}$ puis à faire :\n",
    "$$\n",
    "A^{k+1}\\ \\longleftarrow\\ A^k - \\alpha\\nabla f_{i_k}(A^k),\n",
    "$$\n",
    "où le taux d'apprentissage $\\alpha>0$ est fixé.\n",
    "\n",
    "L'indice $i_k$ tiré aléatoirement à l'étape $k$ l'est indépendemment des indices précédents $i_0,i_1,\\dots,i_{k-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmqPOT9eJ6DC"
   },
   "source": [
    "__Initialisation__. On définit:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "($*$) Un ensemble de coefficients initial sous la forme d'une coef-list _A_ construite au hasard.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "($*$) Un flottant _alpha_ correspondant au taux d'apprentissage ($\\alpha=0.05$ ici).<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "($*$) Un nombre d'époques total _Nepoch_.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "($*$) Un entier _nepoch_ initialisé  à 0 qui va représenter le nombre d'époques effectuées.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "($*$) Un entier _Ndata_ représentant la taille des données.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "($*$) Un entier _nepochplot_ qui indique la fréquence des représentations graphiques au cours des itérations (une représentation graphique toutes les _niterplot_ époques). <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "($*$) Une liste vide *Total_loss* pour stocker l'évolution de l'erreur totale au cours des itérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IdVT9w-hJ6DJ"
   },
   "outputs": [],
   "source": [
    "# Paramètres\n",
    "alpha=0.005\n",
    "Nepoch=300\n",
    "Ndata=data.n\n",
    "nepochplot=25\n",
    "\n",
    "nplot_per_row = 4 # nombre de figure par ligne\n",
    "\n",
    "# Initialisations\n",
    "A=nn.create_rand()\n",
    "nepoch=0\n",
    "Erreur =[nn.total_loss(A,data,pb=pb)]\n",
    "plotpos=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5l_mvC1OJ6Da"
   },
   "source": [
    "__Boucle d'optimisation__. Noter que les deux boucles sur *i\\_* et sur *j\\_* pourraient être rassemblées en une seule boucle. Cette décomposition ne sert qu'à compter des époques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "N49qsCBGJ6Df",
    "outputId": "624cf45f-d914-45bf-c277-c73e96fa9773"
   },
   "outputs": [],
   "source": [
    "# Boucle d'optimisation pour la methode du gradient stochastique\n",
    "for i_ in range(Nepoch):\n",
    "    nepoch+=1\n",
    "    for j_ in range(Ndata):\n",
    "        i = nprd.randint(Ndata)\n",
    "        x, y = data.X[i], data.Y[i]\n",
    "        dA = nn.descent(A, x, y, alpha=alpha, pb=pb)\n",
    "        nn.add(A, dA, output=False)\n",
    "    \n",
    "    # calcul de l'erreur et représentations graphiques\n",
    "    if not nepoch%nepochplot:\n",
    "        error = nn.total_loss_and_prediction(A,data,pb=pb)\n",
    "        Erreur.append(error)\n",
    "        if not plotpos: plt.figure(figsize=(16,4))\n",
    "        plotpos+=1\n",
    "        plt.subplot(1,nplot_per_row,plotpos)\n",
    "        data.show_class(pred=True)\n",
    "        nn.show_pred(A)\n",
    "        pb.show_border('k--')\n",
    "        plt.title(f\"epoch: {nepoch}, Tot. loss: {error:1.5e}.\", fontsize=12)\n",
    "        if plotpos==nplot_per_row :  \n",
    "            plt.show()\n",
    "            plotpos=0\n",
    "    else:\n",
    "        error = nn.total_loss(A,data,pb=pb)\n",
    "        Erreur.append(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Étude graphique de l'évolution de l'erreur totale__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Représentations graphiques de l'évolution de l'erreur au cours des époques.\n",
    "nn.show(A)\n",
    "\n",
    "print(f\"Erreur initiale : {Erreur[0]:1.5e}\")\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "debut = 1\n",
    "plt.plot(np.linspace(debut, nepoch-1,nepoch-debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "\n",
    "plt.subplot(122)\n",
    "debut = nepoch//2\n",
    "plt.plot(np.linspace(debut, nepoch-1,nepoch-debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "debut = 3*(nepoch//4)\n",
    "plt.plot(np.linspace(debut, nepoch-1,nepoch - debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "\n",
    "plt.subplot(122)\n",
    "debut = 7*(nepoch//8)\n",
    "plt.plot(np.linspace(debut, nepoch-1,nepoch - debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1.**\n",
    "\n",
    "(a) Commentez l'évolution de l'erreur. Donnez des explications.\n",
    "\n",
    "(b) Changez la valeur du taux d'apprentissage. Observez et commentez. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top : 0px\">\n",
    "    \n",
    "__Solution 1.__\n",
    "La présence d'aléa dans le choix de la fonction $f_{i_k}$ minimisée à l'étape $k$ introduit une compétition entre d'une part le gain moyen dû au fait que la moyenne des $-\\nabla f_i(A)$ est $-\\nabla F(A)$ qui est bien une direction de descente pour $F$ en $A$ et d'autre part l'effet aléatoire dû à la différence entre $\\nabla F(A)$ et $\\nabla f_i(A)$ qu'on peut quantifier par l'écart type\n",
    "$$\n",
    "E(A)=\\left(\\dfrac1{n_d}\\sum_{i=0}^{n_d-1}\\left\\|\\nabla f_i(A)-\\nabla F(A)\\right\\|^2\\right)^{1/2}.\n",
    "$$\n",
    "Cette différence a d'autant plus d'effet qu'on est proche d'un minimiseur local $A^*$. Dans ce cas $\\nabla F(A^k)$ est petit et l'algorithme produit une marche aléatoire des $A^k$ au voisinage de $A^*$, avec en moyenne $\\|A^k-A^*\\|$ de l'ordre de $\\alpha E(A^*)$.\n",
    "\n",
    "On observe trois phases distinctes dans l'évolution de l'erreur.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "(a) Une première phase où l'erreur totale décroît très rapidement (jusqu'autour de la vingt-cinquième itération ici). Dans cette phase le gain moyen domine clairement le bruit.<br>  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "(b) Une seconde phase où l'erreur décroît toujours en moyenne mais plus lentement et avec des oscillations de l'erreur dont l'amplitude est plus importante que le gain qu'il serait encore possible de réaliser sur l'erreur.\n",
    "\n",
    "\n",
    "Les passage d'une phase à l'autre n'est pas brutal et le choix du point de passage d'une phase à l'autre nécessairement arbitraire.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GafO0zXoJ6Cx"
   },
   "source": [
    "<h2><a id='II'></a>\n",
    "2. Réduction du bruit par décroissance du taux d'apprentissage</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#toc\">top</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#0\">0</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#I\">1.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#II\">2.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#III\">3.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#bot\">bot.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour réduire progressivement le bruit, on fait décroître le taux d'apprentissage au cours des _itérations_. L'itération $k$ devient :<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Calculer le taux d'apprentissage $\\alpha_k$.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "Choisir aléatoirement (et uniformément) $i\\in\\{0,\\dots,n_d-1\\}$.<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; \n",
    "Faire :\n",
    "$$\n",
    "A\\ \\longleftarrow\\ A - \\alpha_k\\nabla f_i(A),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous proposons pour $\\alpha_k$ la formule\n",
    "$$\\tag{1}\n",
    "\\alpha_k := \\dfrac{\\lambda\\alpha_0}{\\lambda+k}=\\dfrac{\\alpha_0}{1+ k/\\lambda},\n",
    "$$\n",
    "où les paramètres $\\alpha_0$ et $\\lambda$ sont strictement positifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2.** Donnez des arguments pour le choix des paramètres $\\alpha_0$ et $\\lambda$ (du moins donnez une méthode pour déterminer l'ordre de grandeur de paramètres conduisant à un algorithme efficace). \n",
    "\n",
    "On pourra vérifier le bien fondé de ces choix en testant la méthode (Exercice 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top : 0px\">\n",
    "\n",
    "**Solution 2.** Dans la formule (1), $\\alpha_0$ est la valeur du premier taux d'apprentissage utilisé. Il est donc raissonnable de prendre pour $\\alpha_0$ une valeur qui était efficace pour les premières itérations de la méthode à pas fixe.  Soit, ici,\n",
    "$$\n",
    "\\alpha_0=5\\cdot 10^{-2}.\n",
    "$$\n",
    "\n",
    "Si $\\lambda$ est un entier alors $\\alpha_{\\lambda}=\\alpha_0/2$. C'est à dire que le paramètre $\\lambda$ correspond au nombre d'itérations qu'il faut pour que le taux d'accroissement soit divisé par 2. Plus généralement, pour un entier $m\\ge0$,\n",
    "$$\n",
    "\\alpha_{m\\lambda}=\\dfrac{\\alpha_0}{m+1}.\n",
    "$$\n",
    "Pour profiter pleinement de la phase (a) de convergence rapide de la méthode du gradient stochastique testée dans la partie **1**, il ne faut pas que $\\lambda$ soit trop petit. En notant $n_{\\text{a}}$ le nombre d'itérations de cette phase (a), on demandera $\\lambda\\ge n_{\\text{a}}$.<br>\n",
    "On ne veut pas non plus que $\\lambda$ soit trop grand pour que l'effet de réduction du bruit atténue au plus vite le phénomène de \"marche aléatoire\" de la phase (c).<br>\n",
    "Un choix raisonnable semble de prendre $\\lambda$ du même ordre de grandeur que $n_{\\text{a}}$, disons $\\lambda\\simeq 2n_{\\text{a}}$. Pour le problème particulier que nous traitons, la fin de la phase (a) se situe autour de 25 époques et comme une époque contient 1000 itérations, on a:\n",
    "$$\n",
    "n_{\\text{a}}\\simeq 25\\cdot1000=2.5\\cdot10^4.\n",
    "$$\n",
    "Dans l'implémentation proposée plus bas nous prenons exactement $\\lambda=5\\cdot10^4$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3.** Implémentez la méthode du gradient stochastique avec le taux d'apprentissage variable donné par la formule (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IdVT9w-hJ6DJ"
   },
   "outputs": [],
   "source": [
    "## Solution 3.a\n",
    "# Paramètres\n",
    "alpha0=0.005\n",
    "fact_dec_alpha=1/(50*data.n) # inverse de lambda\n",
    "Nepoch=600\n",
    "Ndata=data.n\n",
    "nepochplot=50\n",
    "\n",
    "nplot_per_row = 4 # nombre de figures par ligne\n",
    "\n",
    "# Initialisations\n",
    "A=nn.create_rand()\n",
    "nepoch=0\n",
    "k=0\n",
    "Erreur =[nn.total_loss(A,data,pb=pb)]\n",
    "plotpos=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "N49qsCBGJ6Df",
    "outputId": "624cf45f-d914-45bf-c277-c73e96fa9773"
   },
   "outputs": [],
   "source": [
    "## Solution 3.b\n",
    "# Boucle d'optimisation pour la methode du GS avec décroissance de alpha\n",
    "for i_ in range(Nepoch):\n",
    "    nepoch+=1\n",
    "    for j_ in range(Ndata):\n",
    "        i = nprd.randint(Ndata)\n",
    "        x, y = data.X[i], data.Y[i]\n",
    "        dA = nn.descent(A, x, y, alpha=alpha0/(1+fact_dec_alpha*k), pb=pb)\n",
    "        nn.add(A, dA, output=False)\n",
    "        k +=1\n",
    "    # calcul de l'erreur et représentations graphiques\n",
    "    if not nepoch%nepochplot:\n",
    "        error = nn.total_loss_and_prediction(A,data,pb=pb)\n",
    "        Erreur.append(error)\n",
    "        if not plotpos: plt.figure(figsize=(16,4))\n",
    "        plotpos+=1\n",
    "        plt.subplot(1,nplot_per_row,plotpos)\n",
    "        data.show_class(pred=True)\n",
    "        nn.show_pred(A)\n",
    "        pb.show_border('k--')\n",
    "        plt.title(f\"ep: {nepoch}, Loss: {error:1.5e}.\", fontsize=12)\n",
    "        if plotpos==nplot_per_row :  plt.show(); plotpos=0\n",
    "    else:\n",
    "        error = nn.total_loss(A,data,pb=pb)\n",
    "        Erreur.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution 3.c\n",
    "## Représentations graphiques de l'évolution de l'erreur au cours des époques.\n",
    "\n",
    "#nn.show(A)\n",
    "\n",
    "print(f\"Erreur initiale : {Erreur[0]:1.5e}\")\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "debut = 1\n",
    "plt.plot(np.linspace(debut, nepoch,nepoch-debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "\n",
    "plt.subplot(122)\n",
    "debut = nepoch//2\n",
    "plt.plot(np.linspace(debut, nepoch,nepoch-debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "debut = 3*(nepoch//4)\n",
    "plt.plot(np.linspace(debut, nepoch,nepoch - debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "\n",
    "plt.subplot(122)\n",
    "debut = 7*(nepoch//8)\n",
    "plt.plot(np.linspace(debut, nepoch,nepoch - debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4.** Commentez les résultats numériques. Observe-t-on les effets décrits en cours ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top : 0px\">\n",
    "\n",
    "__Solution 4.__ On observe deux phénomènes en comparaison de la méthode du gradien stochastique à taux d'apprentissage constant.<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; $\\bullet$ D'une part la décroissance de l'erreur est plus régulière et légèrement plus lente.<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; $\\bullet$ D'autre part, l'erreur continue de décroître: la phase (c) a disparu.\n",
    "    \n",
    "Remarquons aussi que le choix de $\\alpha_k$ pourraît être modulé en fonction des oscillations observées de l'erreur.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GafO0zXoJ6Cx"
   },
   "source": [
    "<h2><a id='III'></a>3. Méthodes des mini-lots (mini-batch method)</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#toc\">top</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#0\">0</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#I\">1.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#II\">2.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#III\">3.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#bot\">bot.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode des mini-lots est une méthode intermédiare entre la méthode du gradient (full batch) et la méthode du gradient stochastique. Au lieu de considérer une seule fonction $f_{i_k}$ à l'étape $k$, on tire un lot de $n_k$ fonctions $f_{i_{k,0}},\\dots,f_{i_{k,n_k-1}}$ et on fait \n",
    "$$\n",
    "A^{k+1}\\ \\longleftarrow\\ A^k-\\alpha_k\\dfrac1{n_k}\\sum_{j=0}^{n_k-1}\\nabla f_{i_{k,j}}(A_k).\n",
    "$$\n",
    "Notez que le calcul des $\\nabla f_{i_{k,j}}(A_k)$ peut être effectué en parallèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5.** \n",
    "\n",
    "(i) Implémentez et testez la méthode de mini-batch sur le problème défini plus haut. On prendra $n_k=n_{\\text{batch}}=30$ (indépendant de $k$) et dans un premier temps un taux d'apprentissage constant. \n",
    "\n",
    "On rappelle que le nombre d'itérations par époque est $\\lfloor n_d/n_{\\text{batch}}\\rfloor$. \n",
    "\n",
    "(ii) Comparez la méthode de mini-batch et la méthode de gradient stochastique pur. Est-ce que les taux d'apprentissage permettant la convergence sont les mêmes que dans le cas du gradient stochastique pur ? Pour un même taux d'apprentissage, y-a-il une différence dans l'évolution de l'erreur au cour des itérations?\n",
    "\n",
    "(iii) Implémentez la méthode du mini-batch avec $\\alpha_k$ donné par la formule (1).\n",
    "\n",
    "(iv) Quels sonts les conclusions générales que vous tirez du TP ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IdVT9w-hJ6DJ"
   },
   "outputs": [],
   "source": [
    "## Solution 5.i.a\n",
    "# Paramètres\n",
    "alpha0=0.01\n",
    "Nepoch=600\n",
    "Nbatch=30\n",
    "Ndata=data.n\n",
    "nepochplot=50\n",
    "Niter_per_epoch=Ndata//Nbatch\n",
    "fact_dec_alpha=0 \n",
    "#fact_dec_alpha=1/(600*Niter_per_epoch)\n",
    "\n",
    "nplot_per_row = 4 # nombre de figures par ligne\n",
    "\n",
    "# Initialisations\n",
    "A=nn.create_rand()\n",
    "nepoch=0\n",
    "k=0\n",
    "Erreur =[nn.total_loss(A,data,pb=pb)]\n",
    "plotpos=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "N49qsCBGJ6Df",
    "outputId": "624cf45f-d914-45bf-c277-c73e96fa9773"
   },
   "outputs": [],
   "source": [
    "## Solution 5.i.b\n",
    "# Boucle d'optimisation pour la methode du GS avec décroissance de alpha\n",
    "for i_ in range(Nepoch):\n",
    "    nepoch+=1\n",
    "    for j_ in range(Niter_per_epoch):\n",
    "        I = nprd.choice(Ndata, Nbatch)\n",
    "        DA=nn.create_zero()\n",
    "        for i in I:\n",
    "            x, y = data.X[i], data.Y[i]\n",
    "            nn.descent(A, x, y, alpha=1, B=DA, pb=pb)\n",
    "        nn.add(A, DA, c=alpha0/(Nbatch*(1+fact_dec_alpha*k)), output=False)\n",
    "        k +=1\n",
    "    # calcul de l'erreur et représentations graphiques\n",
    "    if not nepoch%nepochplot:\n",
    "        error = nn.total_loss_and_prediction(A,data,pb=pb)\n",
    "        Erreur.append(error)\n",
    "        if not plotpos: plt.figure(figsize=(16,4))\n",
    "        plotpos+=1\n",
    "        plt.subplot(1,nplot_per_row,plotpos)\n",
    "        data.show_class(pred=True)\n",
    "        nn.show_pred(A)\n",
    "        pb.show_border('k--')\n",
    "        plt.title(f\"ep: {nepoch}, Loss: {error:1.5e}.\", fontsize=12)\n",
    "        if plotpos==nplot_per_row :  plt.show(); plotpos=0\n",
    "    else:\n",
    "        error = nn.total_loss(A,data,pb=pb)\n",
    "        Erreur.append(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution 5.i.c\n",
    "## Représentations graphiques de l'évolution de l'erreur au cours des époques.\n",
    "\n",
    "#nn.show(A)\n",
    "\n",
    "print(f\"Erreur initiale : {Erreur[0]:1.5e}\")\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "debut = 1\n",
    "plt.plot(np.linspace(debut, nepoch,nepoch-debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "\n",
    "plt.subplot(122)\n",
    "debut = nepoch//2\n",
    "plt.plot(np.linspace(debut, nepoch,nepoch-debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(121)\n",
    "debut = 3*(nepoch//4)\n",
    "plt.plot(np.linspace(debut, nepoch,nepoch - debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "\n",
    "plt.subplot(122)\n",
    "debut = 7*(nepoch//8)\n",
    "plt.plot(np.linspace(debut, nepoch,nepoch - debut + 1),Erreur[debut:])\n",
    "plt.title(\"Erreur en fonction du nombre d'époques\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top : 0px\">\n",
    "    \n",
    "**Solution 5.ii.** \n",
    "\n",
    "On remarque que la méthode de mini-lots reste stable avec un taux d'apprentissage plus important que pour la méthode de gradient stochastique pure. Cela est dû au fait que la moyenne\n",
    "$$\n",
    "-\\dfrac1{n_k}\\sum_{j=0}^{n_k-1}\\nabla f_{i_{k,j}}(A^k)\n",
    "$$\n",
    "est plus souvent une direction de descente pour $F(A^k)$ que le seul $-\\nabla f_{i_k}(A^k)$. \n",
    "\n",
    "Pour les mêmes raisons les oscillations de l'erreur sont moins importantes dans cas de la méthode de mini-lots.\n",
    "\n",
    "Cela présente un inconvénient. En effet les oscillations aléatoires permettent d'explorer le paysage énergétique. On pourra remarquer que dans le cas de la méthode des mini-lots, à la fin de la phase de décroissance rapide, l'erreur est plus élevée que pour la méthode de gradientstochastique pure.\n",
    "     </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top : 0px\">\n",
    "    \n",
    "**Solution 5.iii.** Il suffit de changer la définition de _fact_dec_alpha_ dans la partie _\\# Paramètres_ en commentant la ligne courante et décommantant la suivante.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top : 0px\">\n",
    "    \n",
    "**Solution 5.iv.** \n",
    "\n",
    "Nous avons vu deux méthodes de réduction du bruit : la méthode des taux d'appentissage décroissants et la méthode des mini-lots. Ces deux méthodes peuvent être avantageusement combinées. \n",
    "\n",
    "Dans le cas de la méthode des mini-lots, le calcul de la direction de descente peut être parallélisé.\n",
    "\n",
    "Les méthodes de réduction de bruit limitent l'exploration du paysage énergétique. Il ne faut donc pas que la réduction soit trop forte ou intervienne trop tôt.\n",
    "\n",
    "\n",
    "Il faut préter attention au choix du taux d'apprentissage initial $\\alpha_0$. Avec un taux trop grand, l'algorithme est instable. Avec un taux trop petit la convergence est très lente. L'intervalle de taux initiaux efficaces dépend du problème, du type de réseau de neurones considérés et de la taille des mini-lots.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#toc\">top</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#0\">0</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#I\">1.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#II\">2.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#III\">3.</a>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<a href=\"#bot\">bot.</a>\n",
    "<a id='bot'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "GafO0zXoJ6Cx",
    "5l_mvC1OJ6Da",
    "ZzS5-IzwaKn3",
    "89AjhkJ2aKoB"
   ],
   "name": "ToyNN_class.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
