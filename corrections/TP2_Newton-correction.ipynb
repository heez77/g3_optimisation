{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font size=\"6\"> Les méthodes de descente de gradient </font> (deuxième partie)</h1>\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table des matières</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top : 20px\">\n",
    "      <ul>\n",
    "          <li><a href=\"#prelim\">Préliminaires</a></li>\n",
    "          <li><a href=\"#Newton\">La méthode de Newton</a></li>\n",
    "          <li><a href=\"#BFGS\">Une méthode de quasi-Newton (BFGS)</a></li>\n",
    "      </ul>\n",
    "</div>\n",
    "<br>\n",
    "<h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prelim'></a>\n",
    "<h2>Préliminaires</h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer les bibliothèques neecéssaires (_numpy_ et _matplotlib.pyplot_).\n",
    "\n",
    "On définit aussi deux functions pour la visualisation de: *1/* les lignes de niveaux de la fonction ojectif et *2/* le champ de gradients (pour les fonctions objectifs dépendant de deux variables). \n",
    "\n",
    "Il y a aussi un exemples de graphique utilisé pour observer la vitesse de convergence des méthodes d'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def draw_vector_field(F, xmin, xmax, ymin, ymax, N=15):\n",
    "    X = np.linspace(xmin, xmax, N)  # x coordinates of the grid points\n",
    "    Y = np.linspace(ymin, ymax, N)  # y coordinates of the grid points\n",
    "    U, V = F(*np.meshgrid(X, Y))  # vector field\n",
    "    M = np.hypot(U, V)  # compute the norm of (U,V)\n",
    "    M[M == 0] = 1  # avoid division by 0\n",
    "    U /= M  # normalize the u componant\n",
    "    V /= M  # normalize the v componant\n",
    "    return plt.quiver(X, Y, U, V, angles='xy')\n",
    "\n",
    "def level_lines(f, xmin, xmax, ymin, ymax, levels, N=500):\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    y = np.linspace(ymin, ymax, N)\n",
    "    z = f(*np.meshgrid(x, y))\n",
    "    level_l = plt.contour(x, y, z, levels=levels)\n",
    "    #plt.clabel(level_l, levels, fmt='%.1f') \n",
    "\n",
    "f = lambda x, y : np.cosh(x)+ np.sin(x + y)**2\n",
    "df = lambda x, y : np.array([np.sinh(x) + 2*np.cos(x + y)*np.sin(x + y), 2*np.cos(x + y)*np.sin(x + y)])\n",
    "%matplotlib inline\n",
    "level_lines(f, -1.1, 1.1, -1.1, 1.1, np.linspace(1, 3, 10))\n",
    "draw_vector_field(df, -1.1, 1.1, -1.1, 1.1, 10)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot of the values of f along the iterations.\n",
    "N = 10\n",
    "F = 2**(-np.linspace(0,N,N+1))\n",
    "plt.figure()\n",
    "plt.semilogy(range(N + 1), F, '.', linestyle='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Newton'></a>\n",
    "<h2>La méthode de Newton</h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose dans ce TP que $f:\\mathbb{R}^N\\to\\mathbb{R}$ est de classe $C^2$ au moins.\n",
    "\n",
    "La méthode de Newton (ou de Newton-Raphson) est une méthode de descente itérative dans laquelle la direction de descente à l'étape $k$ est choisie de manière à minimiser le développement limité au second ordre de $f$ au point $x^k$, c'est-à-dire\n",
    "$$\n",
    "\\tag{4}\n",
    "m_k(d):=f(x^k) + d\\cdot \\nabla f(x^k) + \\dfrac12 d^T D^2 f(x^k) d.\n",
    "$$\n",
    "Si la matrice (symétrique) $D^2 f(x^k)$ est définie  positive le minimiseur de $m^k$ existe et est unique. On note $H^k$ l'inverse de $D^2 f(x^k)$, $g^k:=\\nabla f(x^k)$ et $d^k$ le minimiseur de (4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 15.*** Exprimez $d^k$ en fonction de $H^k$ et $g^k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Solution:___ $d^k=-H^kg^k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque la direction de descente $d^k$ a été calculée, nous procédons comme dans la méthode de descente de gradient et utilisons une méthode de backtracking ($\\alpha \\leftarrow \\beta\\alpha$) pour trouver $\\alpha_k$ tel que $f(x^k+ \\alpha_k d^k)$ soit  \"suffisamment plus petit\" que $f(x^k)$. Pour quantifier ce \"suffisamment plus petit\", nous utiliserons à nouveau le critère d'Armijo : \n",
    "\n",
    "$$\n",
    "\\tag{5}\n",
    "f(x^k+\\alpha_kd^k)\\, \\le\\, f(x^k) + c\\, \\alpha_k \\left<d^k;g^k\\right>,\n",
    "$$\n",
    "où $c\\in(0,1)$ sera fixé. On utilisera la même méthode de baEt la méthode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, l'algorithme de Newton-Raphson avec une line search par critère d'Armijo se décrit comme ceci : \n",
    "\n",
    "Au départ, on fixe $c,\\beta\\in(0,1)$ et on choisit $x^0\\in\\mathbb{R}^N$.\n",
    "<br> \n",
    "Ensuite pour $k=0,1,\\dots$ jusqu'à convergence, on répète :\n",
    "$$\n",
    "\\left|\n",
    "\\begin{array}{l}\n",
    "\\text{Calculer }d^k,\\\\\n",
    "\\text{Trouver }\\alpha_k>0 \\text{ tel que (5) soit vrai,}\\\\\n",
    "\\text{Faire }x^{k+1}\\,\\leftarrow\\,x^k + \\alpha_k d^k.\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Un point important ici est que, lorsque $x^k$ est suffisamment proche d'un minimiseur local $x^*$ de $f$ tel que $D^2f(x^*)$ soit défini positif, alors le choix $\\alpha_k=1$ donne une convergence quadratique vers $x^*$, _i.e._ \n",
    "$$\n",
    "\\|x^{k+1}-x^*\\|\\leq C \\|x^k - x^*\\|^2.\n",
    "$$\n",
    "Si nous ne voulons pas perdre cette convergence super-linéaire, nous devons choisir $\\alpha_k=1$ dès que $x^k$ est assez proche de $x^*$. Pour cela, nous commençons les itérations de back-tracking avec $\\alpha=1$ et nous choisissons une valeur de $c$   relativement petite dans (5) (par exemple $c=0.1$).\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $\\Lambda>0$. On pose $f_\\Lambda(x,y):=(1-x)^2 + \\Lambda\\,(y-x^2)^2$, pour$(x,y)\\in\\mathbb{R}^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 16.__ Calculez $\\nabla f_\\Lambda(x,y)$. Trouves le(s) minimiseur(s) de $f_\\Lambda$. Tracez quelques lignes de niveau de $f_\\Lambda$ ainsi que le champ vectoriel renormalisé $(1/|\\nabla f_\\Lambda|)\\nabla f_\\Lambda$ pour $\\Lambda=100$. Calculez $D^2 f(x,y)$ et son inverse $H_\\Lambda(x,y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 1000\n",
    "f = lambda x,y : ( x - 1)**2 + Lambda*(y - x**2)**2\n",
    "df = lambda x,y : np.array([2*(x - 1) + 4*Lambda*x*(x**2 - y), 2*Lambda*(y - x**2)])\n",
    "ddf = lambda x,y : np.array([[2 - 4*Lambda*y + 12*Lambda*x**2 , -4*Lambda*x], [-4*Lambda*x, 2*Lambda]])\n",
    "HH = lambda x,y : (1/(4*Lambda - 8*(Lambda**2)*y \n",
    "                      + 8*(Lambda**2)*x**2))*np.array([[2*Lambda, 4*Lambda*x],\n",
    "                                                       [4*Lambda*x, 2 - 4*Lambda*y + 12*Lambda*x**2 ]])\n",
    "\n",
    "%matplotlib inline\n",
    "level_lines(f, .8, 1.2, 0.8, 1.2, np.linspace(0, 30, 80))\n",
    "draw_vector_field(df, .8, 1.2, 0.8, 1.2, 15)\n",
    "plt.plot(1,1,'or')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 17.__ Implémentez la méthode de Newton et appliquez-la à la fonction ci-dessus avec $c=0.1$, $\\beta=0.75$ et $x^0=(0,0)$. Représentez les itérations sur un graphique et tracez $\\ \\log(f_\\Lambda(x^k))\\ $ en fonction de $k$. Commentez les résultats.\n",
    "\n",
    "_Indication:_ Testez d'abord l'algorithme sur la fonction quadratique ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour le test\n",
    "'''\n",
    "f = lambda x,y : ( x - 1)**2 + 2*(y - 1)**2\n",
    "df = lambda x,y : np.array([2*(x - 1) , 4*(y - 1)])\n",
    "#ddf = lambda x,y : np.array([[2  , 0], [0, 2]])\n",
    "HH = lambda x,y : np.array([[.5, 0], [0, .25]])\n",
    "'''\n",
    "Lambda = 1000\n",
    "f = lambda x,y : ( x - 1)**2 + Lambda*(y - x**2)**2\n",
    "df = lambda x,y : np.array([2*(x - 1) + 4*Lambda*x*(x**2 - y), 2*Lambda*(y - x**2)])\n",
    "ddf = lambda x,y : np.array([[2 - 4*Lambda*y + 12*Lambda*x**2 , \n",
    "                              -4*Lambda*x],[-4*Lambda*x, 2*Lambda]])\n",
    "HH = lambda x,y : (1/(4*Lambda - 8*(Lambda**2)*y \n",
    "                    + 8*(Lambda**2)*x**2))*np.array([[2*Lambda, 4*Lambda*x],\n",
    "                    [4*Lambda*x, 2 - 4*Lambda*y + 12*Lambda*x**2 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "c, beta = .1, .75\n",
    "epsilon = 1e-13\n",
    "itermax = 200\n",
    "iter_ls_max = 40\n",
    "\n",
    "## initialization \n",
    "x, y = 0.2, 0.5\n",
    "w = np.array([x, y])\n",
    "fw = f(x, y)\n",
    "g = np.array(df(x, y))\n",
    "ng=np.hypot(g[0], g[1])\n",
    "ng0=ng\n",
    "iteration = 0\n",
    "\n",
    "## For the post-analysis\n",
    "flag = 'OK'\n",
    "W, F, NG, Alpha =[w], [fw], [ng], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optmization loop\n",
    "while (iteration < itermax):\n",
    "    iteration += 1\n",
    "    d = -np.dot(HH(x, y), g)\n",
    "    dx, dy = d[0], d[1]\n",
    "    if ng < epsilon*ng0 or flag == 'Not OK':\n",
    "        break\n",
    "    mcdg, alpha = -c*np.dot(d,g), 1\n",
    "    new_fw = f(x + alpha*dx, y + alpha*dy) \n",
    "    iter_ls = 0\n",
    "    while (new_fw - fw + alpha*mcdg >=0):\n",
    "        alpha *=beta\n",
    "        new_fw = f(x + alpha*dx, y + alpha*dy)\n",
    "        iter_ls += 1\n",
    "        if (iter_ls>=iter_ls_max):\n",
    "            flag = 'Not OK'\n",
    "            break\n",
    "    x, y, fw = x + alpha*dx, y + alpha*dy, new_fw\n",
    "    g = np.array(df(x, y))\n",
    "    ng=np.hypot(g[0], g[1])\n",
    "    \n",
    "    W.append(np.array([x, y]))\n",
    "    F.append(fw)\n",
    "    NG.append(ng)\n",
    "    Alpha.append(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"flag = {flag}, nombre d'itérations = {iteration}\")    \n",
    "    \n",
    "W = np.array(W)\n",
    "F = np.array(F)\n",
    "\n",
    "# plot the results \n",
    "plt.figure()\n",
    "plt.plot(W[:,0],W[:,1],'.',linestyle='-')\n",
    "level_lines(f, 0, 2, 0, 2, np.linspace(1, 3, 10))\n",
    "draw_vector_field(df, 0 , 2, 0, 2, 10)\n",
    "plt.title(r\"Les itérés $w^k$\", fontsize=15)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of f along the iterations.\n",
    "plt.figure()\n",
    "plt.semilogy(range(len(F)),F,'.',linestyle=':')\n",
    "plt.title(r\"$f(w^k)$ en fonction des itérations\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of norm (grad f) along the iterations.\n",
    "plt.figure()\n",
    "plt.semilogy(range(len(NG)),np.array(NG)/ng0,'.',linestyle=':')\n",
    "plt.title(r\"$\\|\\nabla f(w^k)\\|/\\|\\nabla f(w^0)\\|$ en fonction des itérations\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of the step alpha along the iterations.\n",
    "plt.figure()\n",
    "plt.plot(range(len(Alpha)),Alpha,'.',linestyle=':')\n",
    "plt.title(r\"$\\alpha$ en fonction des itérations\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Commentaires__ : On observe bien les deux comportements de la méthode de Newton, d'abord une approche du minimiseur assez lente, ensuite une convergence extrèmement rapide, qui correspond à l'acceptation systématique du pas $\\alpha=1$ par le critère d'Armijo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BFGS'></a>\n",
    "<h2> Une méthode de quasi-Newton (BFGS)</h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque le nombre de paramètres est important comme il est habituel en Machine Learning, le calcul des matrices hessiennes $D^2f(x^k)$ et la résolution des systèmes linéaires $D^2f(x^k) d^k=-g^k$ peuvent être trop coûteux. Cependant, il est souvent encore possible d'obtenir une convergence superlinéaire en remplaçant $[D^2f(x^k)]^{-1}$ par une approximation moins gourmande à calculer qu'on notera $H^k$. Il existe plusieurs algorithmes basés sur cette idée. Nous présentons l'une des plus populaires : la méthode BFGS du nom de leurs découvreurs (Broyden, Fletcher, Goldfarb et Shanno). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Description de la méthode__ : Supposons qu'à l'étape $k$ nous ayons une approximation définie positive symétrique $H^k$ de $\\left[D^2f(x^k)\\right]^{-1}$. On note $B^k$ son inverse (qui est une approximation de $D^2f(x^k)$). Comme ci-dessus, nous définissons notre direction de descente $d^k$ comme le minimiseur de\n",
    "$$\n",
    "f(x^k) + d\\cdot \\nabla f(x^k) + \\dfrac12 d^T B^k d.\n",
    "$$\n",
    "Cela conduit à la formule :\n",
    "$$\n",
    "d^k = -\\left[B^k\\right]^{-1} \\nabla f(x^k) = - H^k g^k. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche ensuite $\\alpha_k$ satisfaisant (5) par la méthode de ``backtracking\", toujours avec $\\alpha=1$ et on pose\n",
    "$$\n",
    "x^{k+1} := x^k +\\alpha_k d^k.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous avons besoin de calculer approximation $H^{k+1}$ de $\\left[D^2f(x^{k+1})\\right]^{-1}$. Pour cela, rappelons que nous voulons\n",
    "$$\n",
    "\\tilde m_{k+1} (d):= f(x^{k+1}) + g^{k+1}\\cdot d +\\dfrac 12 d^T B^{k+1} d,\n",
    "$$\n",
    "soit une approximation de\n",
    "$$\n",
    "\\overline m_{k+1}(d):= f(x^{k+1} + d).\n",
    "$$\n",
    "Nous avons déjà par construction,\n",
    "$$\n",
    "\\tilde m_{k+1}(0)=\\overline m_{k+1}(0)=f(x^{k+1})\\qquad\\text{et}\\qquad \\nabla \\tilde m_{k +1}(0)=\\nabla \\overline m_{k+1}(0)=g(x^{k+1}).\n",
    "$$\n",
    "Nous appliquons la nouvelle condition\n",
    "$$\n",
    "\\nabla m_{k+1}(-\\tau_k d^k)=\\nabla \\overline m_{k+1}(-\\tau_k d^k)=g^k.\n",
    "$$\n",
    "\n",
    "En notant $a^k:=g^{k+1}-g^k$ et $b^k:=\\tau^kd^k=x^{k+1}-x^k$, cela équivaut à $B^{k+1}b^k=a^k$. En supposant que $B^{k+1}$ est inversible, cela équivaut à demander que $H^{k+1}$ soit solution de\n",
    "$$\n",
    "\\tag{6}\n",
    "Ha^k=b^k.\n",
    "$$\n",
    "Une condition nécessaire et suffisante pour que (6) admette une solution symétrique définie positive $H$ est :\n",
    "$$\n",
    "\\tag{7}\n",
    "\\left<a^k;b^k\\right> >0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ne voulons pas perdre toute l'information déjà contenue dans $H^k$, donc, en supposant que (7) soit vraie, nous choisissons une solution de (6) aussi proche que possible de $H^k$. Un choix populaire consiste à définir :\n",
    "$$\n",
    "\\tag{8}\n",
    "H^{k+1} := \\left(I-\\rho_k b^k\\otimes a^k\\right) H^k \\left(I-\\rho_k a^k\\otimes b^k\\right) + \\rho_k b^k\\otimes b^k,\\quad\\text{ avec }\\quad \\rho_k:=\\dfrac1{\\left<a^k;b^k\\right>}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 18.__ Vérifiez que la formule (8) donne bien une solution à (6). Vérifiez que $H^{k+1}$ ainsi définie est une matrice symétrique définie positive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Solution:___\n",
    "En supposant que $H^k$ soit définie positive et que (7) soit vraie, il est facile de vérifier que $H^{k+1}$ est également définie positive. Tout d'abord, pour $d\\in \\mathbb{R}^N$, en notant $w=d-\\rho_k \\left<b^k;d\\right>a^k$, nous calculons,\n",
    "\n",
    "$$\n",
    "  d^TH^{k+1}d = w^TH^kw + \\rho_k \\left<d;b^k\\right>^2\\ \\geq \\ 0.\n",
    "$$\n",
    "\n",
    "Ensuite, en utilisant la même formule, nous voyons que $d^TH^{k+1}d=0$ implique : (a) $w=0$ et (b) $\\left<d;b^k\\right>=0$. Le point (a) entraîne que $d=\\lambda a^k$ pour un $\\lambda\\in \\mathbb{R}$ et avec (b) on déduit $\\lambda\\left<a^k;b^k\\right>=0$. Donc $\\lambda=0$ et $d=0$. Cela prouve que $H^{k+1}$ est définie positive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 19.__ Implémentez la méthode BFGS et appliquez-la à la fonction ci-dessus avec $c = 0.1$, $\\beta=0.75$ et $x^0=(0,0)$. Comme premier approximation de $D^2f(x^0)$ on prendra $H^0=I$.\n",
    "\n",
    "Représentez les itérations sur un graphique et tracez $\\ \\log(f(x^k))\\ $ en fonction de $k$. Observez et commentez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 20.__ Est-ce que $H^k$ converge vers $[D^2 f(x^*)]^{-1}$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data of the optimization problems\n",
    "Lambda = 1000\n",
    "f = lambda x,y : ( x - 1)**2 + Lambda*(y - x**2)**2\n",
    "df = lambda x,y : np.array([2*(x - 1) + 4*Lambda*x*(x**2 - y), 2*Lambda*(y - x**2)])\n",
    "ddf = lambda x,y : np.array([[2 - 4*Lambda*y + 12*Lambda*x**2 , \n",
    "                              -4*Lambda*x],[-4*Lambda*x, 2*Lambda]])\n",
    "HH = lambda x,y : (1/(4*Lambda - 8*(Lambda**2)*y \n",
    "                    + 8*(Lambda**2)*x**2))*np.array([[2*Lambda, 4*Lambda*x],\n",
    "                    [4*Lambda*x, 2 - 4*Lambda*y + 12*Lambda*x**2 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters of the algorithm\n",
    "c, beta = .1, .75\n",
    "epsilon = 1e-8\n",
    "nitermax = 200\n",
    "niter_lsmax = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialization \n",
    "I = np.identity(2)\n",
    "x, y = 0, .5\n",
    "w = np.array([x, y])\n",
    "\n",
    "fw = f(x, y)\n",
    "g = np.array(df(x, y))\n",
    "ng = np.hypot(g[0], g[1])\n",
    "ng0=ng\n",
    "H = np.identity(2)\n",
    "\n",
    "niter = 0\n",
    "flag = 'OK'\n",
    "\n",
    "W, F, NG, Alpha =[w], [fw], [ng], [] \n",
    "ErrH = [np.linalg.norm(H-HH(x,y), ord = 'fro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optmization loop\n",
    "while (niter < nitermax):\n",
    "    if ng < epsilon*ng0 or flag =='Not OK': break\n",
    "    d = -H@g\n",
    "    dx, dy = d[0], d[1]\n",
    "    new_fw = f(x + dx, y + dy) \n",
    "    alpha, niter_ls = 1, 0\n",
    "    fact = c*np.dot(d,g)\n",
    "    while (new_fw - fw - alpha*fact>=0):\n",
    "        alpha *= beta\n",
    "        new_fw = f(x + alpha*dx, y + alpha*dy)\n",
    "        niter_ls += 1\n",
    "        if (niter_ls>=niter_lsmax):\n",
    "            flag = 'Not OK'\n",
    "            break\n",
    "    x, y, fw = x + alpha*dx, y + alpha*dy, new_fw  \n",
    "    old_g, g = g, np.array(df(x, y)) \n",
    "    ng = np.hypot(g[0], g[1])\n",
    "    a, b = g - old_g, np.array([alpha*dx, alpha*dy])\n",
    "    a_dot_b = np.dot(a,b)\n",
    "    if a_dot_b<=0:\n",
    "        flag = 'Not OK'\n",
    "        break\n",
    "    rho=1/a_dot_b\n",
    "    H = np.dot(np.dot( \n",
    "        I - rho*np.tensordot(b,a,0), H), \n",
    "        I - rho*np.tensordot(a,b,0) \n",
    "        ) + rho*np.tensordot(b,b,0) \n",
    "    \n",
    "    ErrH.append(np.linalg.norm(H-HH(x,y)\n",
    "                        /np.linalg.norm(HH(x,y)), ord = 'fro'))\n",
    "    W.append(np.array([x, y]))\n",
    "    F.append(fw)\n",
    "    NG.append(ng)\n",
    "    Alpha.append(alpha)\n",
    "    niter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"flag = {flag}, nombre d'itérations = {niter}\")    \n",
    "    \n",
    "# plot the results \n",
    "xmin, xmax, ymin, ymax = -.8, 1.8, -.5, 1.5\n",
    "plt.figure()\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import itertools as it\n",
    "l =len(W)\n",
    "colors = it.cycle(cm.rainbow(np.linspace(0, 1, l)))\n",
    "for w in W :\n",
    "    plt.plot(w[0], w[1], '.', color=next(colors))\n",
    "W = np.array(W)\n",
    "plt.plot(W[:,0],W[:,1],linestyle=':')    \n",
    "level_lines(f, xmin, xmax, ymin, ymax, np.linspace(1, 100, 5))\n",
    "draw_vector_field(df, xmin, xmax, ymin, ymax, 15)\n",
    "plt.title(r'Les itérés $w^k$', fontsize=15)\n",
    "plt.xlabel(r'x')\n",
    "plt.ylabel(r'y')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of f along the iterations.\n",
    "plt.figure()\n",
    "plt.semilogy(range(len(F)),F,'.',linestyle=':')\n",
    "plt.title(r\"$f(w^k)$ en fonction des itérations\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of norm (grad f) along the iterations.\n",
    "plt.figure()\n",
    "plt.semilogy(range(len(NG)),np.array(NG)/ng0,'.',linestyle=':')\n",
    "plt.title(r\"$\\|\\nabla f(w^k)\\|/\\|\\nabla f(w^0)\\|$ en fonction des itérations\", fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of ||Happrox-Htrue|| along the iterations.\n",
    "plt.figure()\n",
    "plt.semilogy(range(len(ErrH)),ErrH,'.',linestyle=':')\n",
    "plt.title(r'$|H^k - [D^2f(w^k)]^{-1}|$')\n",
    "plt.xlabel('niter')\n",
    "plt.show()\n",
    "\n",
    "# plot of the step alpha along the iterations.\n",
    "plt.figure()\n",
    "plt.plot(range(len(Alpha)),Alpha,'.')\n",
    "plt.title(r\"$\\alpha$ en fonction des itérations\")\n",
    "plt.xlabel('niter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe à nouveau deux comportement successifs de la convergence. Une convergence relativemente puis une convergence très rapide en 4 ou cinq pas qui est concomitante de l'acceptation de $\\alpha=1$ par le critère d'Armijo.\n",
    "\n",
    "La méthode nécessite deux fois plus d'itérations que la méthode de Newton pour la précision demandée. Cependant, ici on n'a pas à résoudre le système linéaire $D^2f(w^k) d^k =-g^k$ à chaque itération. Cette résolution (très coûteuse pour les grands systèmes mal conditionnés) est le défaut de la méthode de Newton.\n",
    "\n",
    "La matrice $H^k$ \"approchant\" $[D^2f(w^k)]^{-1}$ ne converge pas vers $[D^2f(w^*)]^{-1}$. En fait, on a seulement une approximation dans la direction $w^k-w^*$, \n",
    "$$\n",
    "H^k(w^k-w^*)\\simeq [D^2f(w^*)]^{-1}(w^k-w^*).\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
