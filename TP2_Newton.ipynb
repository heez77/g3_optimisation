{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><font size=\"6\"> Les méthodes de descente de gradient </font> (deuxième partie)</h1>\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table des matières</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top : 20px\">\n",
    "      <ul>\n",
    "          <li><a href=\"#prelim\">Préliminaires</a></li>\n",
    "          <li><a href=\"#Newton\">La méthode de Newton</a></li>\n",
    "          <li><a href=\"#BFGS\">Une méthode de quasi-Newton (BFGS)</a></li>\n",
    "      </ul>\n",
    "</div>\n",
    "<br>\n",
    "<h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prelim'></a>\n",
    "<h2>Préliminaires</h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer les bibliothèques neecéssaires (_numpy_ et _matplotlib.pyplot_).\n",
    "\n",
    "On définit aussi deux functions pour la visualisation de: *1/* les lignes de niveaux de la fonction ojectif et *2/* le champ de gradients (pour les fonctions objectifs dépendant de deux variables). \n",
    "\n",
    "Il y a aussi un exemple de graphique pour observer la vitesse de convergence des méthodes d'optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def draw_vector_field(F, xmin, xmax, ymin, ymax, N=15):\n",
    "    X = np.linspace(xmin, xmax, N)  # x coordinates of the grid points\n",
    "    Y = np.linspace(ymin, ymax, N)  # y coordinates of the grid points\n",
    "    U, V = F(*np.meshgrid(X, Y))  # vector field\n",
    "    M = np.hypot(U, V)  # compute the norm of (U,V)\n",
    "    M[M == 0] = 1  # avoid division by 0\n",
    "    U /= M  # normalize the u componant\n",
    "    V /= M  # normalize the v componant\n",
    "    return plt.quiver(X, Y, U, V, angles='xy')\n",
    "\n",
    "def level_lines(f, xmin, xmax, ymin, ymax, levels, N=500):\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    y = np.linspace(ymin, ymax, N)\n",
    "    z = f(*np.meshgrid(x, y))\n",
    "    level_l = plt.contour(x, y, z, levels=levels)\n",
    "    #plt.clabel(level_l, levels, fmt='%.1f') \n",
    "\n",
    "f = lambda x, y : np.cosh(x)+ np.sin(x + y)**2\n",
    "df = lambda x, y : np.array([np.sinh(x) + 2*np.cos(x + y)*np.sin(x + y), 2*np.cos(x + y)*np.sin(x + y)])\n",
    "%matplotlib inline\n",
    "level_lines(f, -1.1, 1.1, -1.1, 1.1, np.linspace(1, 3, 10))\n",
    "draw_vector_field(df, -1.1, 1.1, -1.1, 1.1, 10)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of f along the iterations.\n",
    "N = 10\n",
    "F = 2**(-np.linspace(0,N,N+1))\n",
    "plt.figure()\n",
    "plt.semilogy(range(N + 1), F, '.', linestyle='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Newton'></a>\n",
    "<h2>La méthode de Newton</h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose dans ce TP que $f:\\mathbb{R}^N\\to\\mathbb{R}$ est de classe $C^2$ au moins.\n",
    "\n",
    "La méthode de Newton (ou de Newton-Raphson) est une méthode de descente itérative dans laquelle la direction de descente à l'étape $k$ est choisie de manière à minimiser le développement limité au second ordre de $f$ au point $x^k$, c'est-à-dire\n",
    "$$\n",
    "\\tag{4}\n",
    "m_k(d):=f(x^k) + d\\cdot \\nabla f(x^k) + \\dfrac12 d^T D^2 f(x^k) d.\n",
    "$$\n",
    "Si la matrice (symétrique) $D^2 f(x^k)$ est définie  positive le minimiseur de $m^k$ existe et est unique. On note $H^k$ l'inverse de $D^2 f(x^k)$, $g^k:=\\nabla f(x^k)$ et $d^k$ le minimiseur de (4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "***Question 15.*** Exprimez $d^k$ en fonction de $H^k$ et $g^k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Réponse</u> : Pour trouver $d^k$ on cherche \n",
    "\n",
    "$\\dfrac{\\partial m_k}{\\partial d}(d^k)=0$\n",
    "\n",
    "Or, $\\dfrac{\\partial m_k}{\\partial d}(d)=g^k + D^k d$\n",
    "\n",
    "Donc $d^k = -H^k g^k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $\\Lambda>0$. On pose $f_\\Lambda(x,y):=(1-x)^2 + \\Lambda\\,(y-x^2)^2$, pour $(x,y)\\in\\mathbb{R}^2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 16.__ Calculez $\\nabla f_\\Lambda(x,y)$. Trouves le(s) minimiseur(s) de $f_\\Lambda$. Tracez quelques lignes de niveau de $f_\\Lambda$ ainsi que le champ vectoriel renormalisé $(1/|\\nabla f_\\Lambda|)\\nabla f_\\Lambda$ pour $\\Lambda=100$. Calculez $D^2 f(x,y)$ et son inverse $H_\\Lambda(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Réponse</u> :\n",
    "\n",
    "$$\\nabla f_{\\Lambda}(x,y) = 2\\binom{-((1-x)+2x\\Lambda(y-x^2)}{\\Lambda(y-x^2)}$$\n",
    "$$D^2f_{\\Lambda}(x,y) = 2\\binom{1-2\\Lambda(y-x^2) + 4x^2 \\quad -2\\Lambda x}{-2\\Lambda x \\quad \\quad \\quad \\quad \\quad \\quad \\Lambda}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution \n",
    "Lambda = 100\n",
    "f = lambda x,y : ( x - 1)**2 + Lambda*(y - x**2)**2\n",
    "df = lambda x,y : np.array([2*(x-1)+4*Lambda*x*(x**2-y),2*Lambda*(y-x**2)])\n",
    "ddf = lambda x,y : np.array([[2+4*Lambda*(3*x**2-y), -4*Lambda*x], [-4*Lambda*x, 2*Lambda]])\n",
    "HH = lambda x,y: np.linalg.inv(ddf(x,y))\n",
    "\n",
    "level_lines(f, .8, 1.2, 0.8, 1.2, np.linspace(0, 30, 80))\n",
    "draw_vector_field(df, .8, 1.2, 0.8, 1.2, 15)\n",
    "plt.plot(1,1,'or')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 17.__ Implémentez la méthode de Newton et appliquez-la à la fonction ci-dessus avec $c=0.1$, $\\beta=0.75$ et $x^0=(0,0)$. Représentez les itérations sur un graphique et tracez $\\ \\log(f_\\Lambda(x^k))\\ $ en fonction de $k$. Commentez les résultats.\n",
    "\n",
    "_Indication:_ Testez d'abord l'algorithme sur la fonction quadratique ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x,y : ( x - 1)**2 + 2*(y - 1)**2\n",
    "df = lambda x,y : np.array([2*(x - 1) , 4*(y - 1)])\n",
    "#ddf = lambda x,y : np.array([[2  , 0], [0, 2]])\n",
    "HH = lambda x,y : np.array([[.5, 0], [0, .25]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x,y : ( x - 1)**2 + Lambda*(y - x**2)**2\n",
    "df = lambda x,y : np.array([2*(x-1)+4*Lambda*x*(x**2-y),2*Lambda*(y-x**2)])\n",
    "ddf = lambda x,y : np.array([[2+4*Lambda*(3*x**2-y), -4*Lambda*x], [-4*Lambda*x, 2*Lambda]])\n",
    "HH = lambda x,y: np.linalg.inv(ddf(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "c, beta = .1, .9\n",
    "epsilon = 1e-8\n",
    "itermax = 200\n",
    "iter_ls_max = 40\n",
    "\n",
    "## initialization \n",
    "iter = 0\n",
    "x, y, alpha = 0, 0, 1. \n",
    "fz = f(x,y)\n",
    "W, F =[np.array([x, y])], [fz]\n",
    "flag = 'OK'\n",
    "\n",
    "## Optmization loop\n",
    "while (iter < itermax):\n",
    "    g = df(x,y)\n",
    "    hh = HH(x,y)\n",
    "    dx,dy = -np.dot(hh, g)\n",
    "    d = np.hypot(dx, dy)\n",
    "    if d < epsilon or flag == 'Not OK':\n",
    "        break\n",
    "   \n",
    "    new_fz = f(x + alpha*dx, y + alpha*dy) \n",
    "    iter_ls = 0\n",
    "    dd = d**2\n",
    "    while (new_fz - fz + c*alpha*dd >=0):\n",
    "        alpha *= beta\n",
    "        new_fz = f(x + alpha*dx, y + alpha*dy)\n",
    "        iter_ls += 1\n",
    "        if (iter_ls>=iter_ls_max):\n",
    "            flag = 'Not OK'\n",
    "            break\n",
    "    #print(\"d = \" + str(d) + \", f(z) - 1 =\" + str(fz-1) + \", t= \" +str(t))\n",
    "    x, y, fz = x + alpha*dx, y + alpha*dy, new_fz\n",
    "    W.append(np.array([x, y]))\n",
    "    F.append(fz)\n",
    "    alpha /= beta\n",
    "    iter += 1\n",
    "\n",
    "print('flag = '+flag + ', n_iter = ' + str(iter))    \n",
    "W = np.array(W)\n",
    "F = np.array(F)\n",
    "\n",
    "print(f\"On trouve comme valeur de (x,y) = ({np.round(x,3)},{np.round(y,3)})\")\n",
    "\n",
    "# plot the results \n",
    "plt.figure()\n",
    "plt.plot(W[:,0],W[:,1],'.',linestyle='-')\n",
    "level_lines(f, 0, 2, 0, 2, np.linspace(1, 3, 10))\n",
    "draw_vector_field(df, 0 , 2, 0, 2, 10)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of f along the iterations.\n",
    "\n",
    "plt.figure() # NEW\n",
    "plt.plot(np.arange(0, iter+1),np.log(F),'.',linestyle='-') # NEW\n",
    "plt.ylabel('log(f(x,y))') # NEW\n",
    "plt.xlabel('nb_iterations') # NEW\n",
    "plt.title('Evolution of log(f) along the iterations') # NEW\n",
    "plt.show() # NEW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Interprétation</u> : L'algorithme converge vers une solution : (x,y) = (1.0,1.0). On remarque que cette convergence se traduit bien par une diminution de f(x,y) à chaque itération."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BFGS'></a>\n",
    "<h2> Une méthode de quasi-Newton (BFGS)</h2>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque le nombre de paramètres est important comme il est habituel en Machine Learning, le calcul des matrices hessiennes $D^2f(x^k)$ et la résolution des systèmes linéaires $D^2f(x^k) d^k=-g^k$ peuvent être trop coûteux. Cependant, il est souvent encore possible d'obtenir une convergence superlinéaire en remplaçant $[D^2f(x^k)]^{-1}$ par une approximation moins gourmande à calculer qu'on notera $H^k$. Il existe plusieurs algorithmes basés sur cette idée. Nous présentons l'une des plus populaires : la méthode BFGS du nom de leurs découvreurs (Broyden, Fletcher, Goldfarb et Shanno)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Description de la méthode__ : Supposons qu'à l'étape $k$ nous ayons une approximation définie positive symétrique $H^k$ de $\\left[D^2f(x^k)\\right]^{-1}$. On note $B^k$ son inverse (qui est une approximation de $D^2f(x^k)$). Comme ci-dessus, nous définissons notre direction de descente $d^k$ comme le minimiseur de\n",
    "$$\n",
    "f(x^k) + d\\cdot \\nabla f(x^k) + \\dfrac12 d^T B^k d.\n",
    "$$\n",
    "Cela conduit à la formule :\n",
    "$$\n",
    "d^k = -\\left[B^k\\right]^{-1} \\nabla f(x^k) = - H^k g^k. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On cherche ensuite $\\alpha_k$ satisfaisant (5) par la méthode de ``backtracking\", toujours avec $\\alpha=1$ et on pose\n",
    "$$\n",
    "x^{k+1} := x^k +\\alpha_k d^k.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous avons besoin de calculer approximation $H^{k+1}$ de $\\left[D^2f(x^{k+1})\\right]^{-1}$. Pour cela, rappelons que nous voulons\n",
    "$$\n",
    "\\tilde m_{k+1} (d):= f(x^{k+1}) + g^{k+1}\\cdot d +\\dfrac 12 d^T B^{k+1} d,\n",
    "$$\n",
    "soit une approximation de\n",
    "$$\n",
    "\\overline m_{k+1}(d):= f(x^{k+1} + d).\n",
    "$$\n",
    "Nous avons déjà par construction,\n",
    "$$\n",
    "\\tilde m_{k+1}(0)=\\overline m_{k+1}(0)=f(x^{k+1})\\qquad\\text{et}\\qquad \\nabla \\tilde m_{k +1}(0)=\\nabla \\overline m_{k+1}(0)=g(x^{k+1}).\n",
    "$$\n",
    "Nous appliquons la nouvelle condition\n",
    "$$\n",
    "\\nabla m_{k+1}(-\\tau_k d^k)=\\nabla \\overline m_{k+1}(-\\tau_k d^k)=g^k.\n",
    "$$\n",
    "\n",
    "En notant $a^k:=g^{k+1}-g^k$ et $b^k:=\\tau^kd^k=x^{k+1}-x^k$, cela équivaut à $B^{k+1}b^k=a^k$. En supposant que $B^{k+1}$ est inversible, cela équivaut à demander que $H^{k+1}$ soit solution de\n",
    "$$\n",
    "\\tag{6}\n",
    "Ha^k=b^k.\n",
    "$$\n",
    "Une condition nécessaire et suffisante pour que (6) admette une solution symétrique définie positive $H$ est :\n",
    "$$\n",
    "\\tag{7}\n",
    "\\left<a^k;b^k\\right> >0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ne voulons pas perdre toute l'information déjà contenue dans $H^k$, donc, en supposant que (7) soit vraie, nous choisissons une solution de (6) aussi proche que possible de $H^k$. Un choix populaire consiste à définir :\n",
    "$$\n",
    "\\tag{8}\n",
    "H^{k+1} := \\left(I-\\rho_k b^k\\otimes a^k\\right) H^k \\left(I-\\rho_k a^k\\otimes b^k\\right) + \\rho_k b^k\\otimes b^k,\\quad\\text{ avec }\\quad \\rho_k:=\\dfrac1{\\left<a^k;b^k\\right>}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 18.__ Vérifiez que la formule (8) donne bien une solution à (6). Vérifiez que $H^{k+1}$ ainsi définie est une matrice symétrique définie positive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Réponse</u> : On doit vérifier que $H^{k+1} a^k = b^k$ On a\n",
    "\n",
    "$H^{k+1} a^k = (H^k - \\rho_k b^k\\otimes a^k H^k)(I-\\rho_k a^k\\otimes b^k) a^k + \\rho_k (b^k\\otimes b^k) a^k$\n",
    "\n",
    "$H^{k+1} a^k = H^k a^k + (\\rho_k)^2 (b^k\\otimes a^k) H^k (a^k\\otimes b^k) a^k - \\rho_k (b^k\\otimes a^k) H^k a^k - \\rho_k H^k (a^k\\otimes b^k) a^k + \\rho_k (b^k\\otimes b^k) a^k$\n",
    "\n",
    "$H^{k+1} a^k = b^k + (\\rho_k)^2 (b^k\\otimes a^k) H^k (a^k\\otimes b^k) a^k - \\rho_k (b^k\\otimes a^k) b^k - \\rho_k H^k (a^k\\otimes b^k) a^k + \\rho_k (b^k\\otimes b^k) a^k$\n",
    "\n",
    "Or, on sait qu'en dimension quelconque : $(a^k\\otimes b^k) a^k = (b^k\\otimes a^k) b^k = 0$ par orthogonalité du produit vectoriel. Donc :\n",
    "\n",
    "$H^{k+1} a^k = b^k + \\rho_k (b^k\\otimes b^k) a^k$\n",
    "\n",
    "Or, $(b^k\\otimes b^k) a^k = b^k (b^k \\otimes a^k) = 0$. Donc, on obtient finalement que :\n",
    "\n",
    "$H^{k+1} a^k = b^k$\n",
    "\n",
    "En conclusion, $H^{k+1}$ donne bien une solution à (6). De plus, comme $\\left<a^k;b^k\\right> >0$, alors d'après l'égalité précédente, $H^{k+1}$ est bien définie positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 19.__ Implémentez la méthode BFGS et appliquez-la à la fonction ci-dessus avec $c = 0.1$, $\\beta=0.75$ et $x^0=(0,0)$. Comme premier approximation de $D^2f(x^0)$ on prendra $H^0=I$.\n",
    "\n",
    "Représentez les itérations sur un graphique et tracez $\\ \\log(f(x^k))\\ $ en fonction de $k$. Observez et commentez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 20.__ Est-ce que $H^k$ converge vers $[D^2 f(x^*)]^{-1}$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution\n",
    "\n",
    "## Parameters\n",
    "c, beta = .1, .75\n",
    "epsilon = 1e-8\n",
    "itermax = 200\n",
    "iter_ls_max = 40\n",
    "\n",
    "##\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=\"True\")\n",
    "\n",
    "## initialization \n",
    "iter = 0\n",
    "x, y, alpha = 0.0, 0.0, 1. \n",
    "fz = f(x,y)\n",
    "W, F =[np.array([x, y])], [fz]\n",
    "flag = 'OK'\n",
    "hh = np.identity(2)\n",
    "h_error = []\n",
    "\n",
    "## Optmization loop\n",
    "while (iter < itermax):\n",
    "    g = df(x,y)\n",
    "    # hh = HH(x,y)\n",
    "    hh = hh # NEW\n",
    "    dx,dy = -np.dot(hh, g)\n",
    "    d = np.hypot(dx, dy)\n",
    "    if d < epsilon or flag == 'Not OK':\n",
    "        break\n",
    "   \n",
    "    new_fz = f(x + alpha*dx, y + alpha*dy) \n",
    "    iter_ls = 0\n",
    "    dd = d**2\n",
    "    while (new_fz - fz + c*alpha*dd >=0):\n",
    "        alpha *= beta\n",
    "        new_fz = f(x + alpha*dx, y + alpha*dy)\n",
    "        iter_ls += 1\n",
    "        if (iter_ls>=iter_ls_max):\n",
    "            flag = 'Not OK'\n",
    "            break\n",
    "    #print(\"d = \" + str(d) + \", f(z) - 1 =\" + str(fz-1) + \", t= \" +str(t))\n",
    "    b_k = np.array([[-x, -y]]) # NEW\n",
    "    x, y, fz = x + alpha*dx, y + alpha*dy, new_fz\n",
    "    a_k = np.array([df(x, y) - g]) # NEW\n",
    "    b_k += np.array([[x, y]]) # NEW\n",
    "    hh = np.dot(np.dot(np.identity(2) - (1/np.dot(a_k,b_k.T))*np.dot(b_k.T, a_k), hh), (np.identity(2) - (1/np.dot(a_k,b_k.T))*np.dot(a_k.T, b_k))) + (1/np.dot(a_k,b_k.T))*np.dot(b_k.T, b_k) # NEW\n",
    "    W.append(np.array([x, y]))\n",
    "    F.append(fz)\n",
    "    alpha /= beta\n",
    "    iter += 1\n",
    "    if np.linalg.norm(hh-HH(x,y), 2) < 0.5:\n",
    "        h_error.append(np.array([iter, np.linalg.norm(hh-HH(x,y), 2)])) # NEW\n",
    "\n",
    "print('flag = '+flag + ', n_iter = ' + str(iter))    \n",
    "W = np.array(W)\n",
    "F = np.array(F)\n",
    "h_error = np.array(h_error) # NEW\n",
    "\n",
    "print(f\"On trouve comme valeur de (x,y) = ({np.round(x,3)},{np.round(y,3)})\") # NEW\n",
    "\n",
    "# plot the results \n",
    "plt.figure()\n",
    "plt.plot(W[:,0],W[:,1],'.',linestyle='-')\n",
    "level_lines(f, 0, 2, 0, 2, np.linspace(1, 3, 10))\n",
    "draw_vector_field(df, 0 , 2, 0, 2, 10)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# plot of the values of f along the iterations.\n",
    "\n",
    "plt.figure() # NEW\n",
    "plt.plot(np.arange(0, iter+1),np.log(F),'.',linestyle='-') # NEW\n",
    "plt.ylabel('log(f(x,y))') # NEW\n",
    "plt.xlabel('nb_iterations') # NEW\n",
    "plt.title('Evolution of log(f) along the iterations') # NEW\n",
    "plt.show() # NEW\n",
    "\n",
    "# plot of the values of ||Happrox-Htrue|| along the iterations.\n",
    "plt.figure() # NEW\n",
    "plt.plot(h_error[:,0],h_error[:,1],'.',linestyle='-') # NEW\n",
    "plt.ylabel('h_error') # NEW\n",
    "plt.xlabel('nb_iterations') # NEW\n",
    "plt.title('Evolution of ||Happrox-Htrue|| along the iterations') # NEW\n",
    "plt.show() # NEW\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Interprétation</u> : On remarque que même avec l'approximation de la hessienne, on conserve une convergence vers la solution (x,y) = (1.0, 1.0). En particulier, il faut noter que $H_{approx}$ ne converge pas vers $H_{true}$..."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f35fc4c302fa05141946eeee87a02543093a5f9fe4b255be016c8b1114de3b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
